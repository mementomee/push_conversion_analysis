import os
import sys
# –î–æ–¥–∞—î–º–æ –∫–æ—Ä–µ–Ω–µ–≤—É –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é –ø—Ä–æ–µ–∫—Ç—É –¥–æ sys.path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, project_root)

import pandas as pd
import numpy as np
from typing import Optional, List, Dict, Any
import logging
from datetime import datetime, timedelta
import time
from config.database_config import DatabaseManager
from config.constants import *

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PushDatabase:
    """
    –í–∏—Å–æ–∫–æ—Ä—ñ–≤–Ω–µ–≤–∏–π –∫–ª–∞—Å –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –±–∞–∑–∞–º–∏ –¥–∞–Ω–∏—Ö push-–∞–Ω–∞–ª—ñ–∑—É
    –ù–∞–¥–∞—î –∑—Ä—É—á–Ω—ñ –º–µ—Ç–æ–¥–∏ –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑ –º–æ–∂–ª–∏–≤—ñ—Å—Ç—é –∫–µ—à—É–≤–∞–Ω–Ω—è
    """
    
    def __init__(self, cache_enabled: bool = True):
        """
        –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∑ –æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–∏–º –∫–µ—à—É–≤–∞–Ω–Ω—è–º
        
        Args:
            cache_enabled: –ß–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –∫–µ—à—É–≤–∞–Ω–Ω—è –∑–∞–ø–∏—Ç—ñ–≤
        """
        self.db_manager = DatabaseManager()
        self.cache_enabled = cache_enabled
        self._cache = {}
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó –¥–ª—è –∫–µ—à—É
        os.makedirs('data/cache', exist_ok=True)
    
    def _get_cache_key(self, query: str, params: dict = None) -> str:
        """–ì–µ–Ω–µ—Ä—É—î –∫–ª—é—á –¥–ª—è –∫–µ—à—É–≤–∞–Ω–Ω—è"""
        import hashlib
        cache_string = f"{query}_{str(params)}"
        return hashlib.md5(cache_string.encode()).hexdigest()
    
    def _load_from_cache(self, cache_key: str) -> Optional[pd.DataFrame]:
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –¥–∞–Ω—ñ –∑ –∫–µ—à—É"""
        if not self.cache_enabled:
            return None
            
        cache_file = f"data/cache/{cache_key}.parquet"
        if os.path.exists(cache_file):
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∞—Å —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ñ–∞–π–ª—É (–∫–µ—à –¥—ñ–π—Å–Ω–∏–π 1 –≥–æ–¥–∏–Ω—É)
            file_time = os.path.getmtime(cache_file)
            if time.time() - file_time < 3600:  # 1 –≥–æ–¥–∏–Ω–∞
                logger.info(f"üì¶ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑ –∫–µ—à—É: {cache_key[:8]}...")
                return pd.read_parquet(cache_file)
        return None
    
    def _save_to_cache(self, data: pd.DataFrame, cache_key: str) -> None:
        """–ó–±–µ—Ä—ñ–≥–∞—î –¥–∞–Ω—ñ –≤ –∫–µ—à"""
        if not self.cache_enabled:
            return
            
        cache_file = f"data/cache/{cache_key}.parquet"
        data.to_parquet(cache_file)
        logger.info(f"üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–æ –≤ –∫–µ—à: {cache_key[:8]}...")
    
    def execute_query(self, database: str, query: str, params: dict = None, use_cache: bool = True) -> pd.DataFrame:
        """
        –í–∏–∫–æ–Ω—É—î –∑–∞–ø–∏—Ç –¥–æ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö –∑ –æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–∏–º –∫–µ—à—É–≤–∞–Ω–Ω—è–º
        
        Args:
            database: 'statistic' –∞–±–æ 'keitaro'
            query: SQL –∑–∞–ø–∏—Ç
            params: –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –∑–∞–ø–∏—Ç—É –¥–ª—è –∫–µ—à—É–≤–∞–Ω–Ω—è
            use_cache: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –∫–µ—à —á–∏ –Ω—ñ
        
        Returns:
            DataFrame –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        """
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫–µ—à
        cache_key = self._get_cache_key(query, params) if use_cache else None
        if cache_key:
            cached_data = self._load_from_cache(cache_key)
            if cached_data is not None:
                return cached_data
        
        # –í–∏–∫–æ–Ω—É—î–º–æ –∑–∞–ø–∏—Ç
        try:
            if database == 'statistic':
                client = self.db_manager.connect_statistic()
            elif database == 'keitaro':
                client = self.db_manager.connect_keitaro()
            else:
                raise ValueError(f"–ù–µ–≤—ñ–¥–æ–º–∞ –±–∞–∑–∞ –¥–∞–Ω–∏—Ö: {database}")
            
            logger.info(f"üîç –í–∏–∫–æ–Ω–∞–Ω–Ω—è –∑–∞–ø–∏—Ç—É –¥–æ {database}...")
            start_time = time.time()
            
            df = self.db_manager.query_to_df(client, query)
            
            execution_time = time.time() - start_time
            logger.info(f"‚úÖ –ó–∞–ø–∏—Ç –≤–∏–∫–æ–Ω–∞–Ω–æ –∑–∞ {execution_time:.2f}—Å, –æ—Ç—Ä–∏–º–∞–Ω–æ {len(df)} –∑–∞–ø–∏—Å—ñ–≤")
            
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –≤ –∫–µ—à
            if cache_key and not df.empty:
                self._save_to_cache(df, cache_key)
            
            return df
            
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –∑–∞–ø–∏—Ç—É: {e}")
            raise
    
    def get_push_data(self, 
                     start_date: str = PUSH_START_DATE,
                     end_date: str = PUSH_END_DATE,
                     ab_groups: List[str] = None,
                     countries: List[str] = None) -> pd.DataFrame:
        """
        –û—Ç—Ä–∏–º—É—î –¥–∞–Ω—ñ –ø—Ä–æ push-—Å–ø–æ–≤—ñ—â–µ–Ω–Ω—è –∑ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—î—é
        
        Args:
            start_date: –ü–æ—á–∞—Ç–∫–æ–≤–∞ –¥–∞—Ç–∞ (YYYY-MM-DD)
            end_date: –ö—ñ–Ω—Ü–µ–≤–∞ –¥–∞—Ç–∞ (YYYY-MM-DD)
            ab_groups: –°–ø–∏—Å–æ–∫ A/B –≥—Ä—É–ø –¥–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó
            countries: –°–ø–∏—Å–æ–∫ –∫—Ä–∞—ó–Ω –¥–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó
        
        Returns:
            DataFrame –∑ push-–¥–∞–Ω–∏–º–∏
        """
        # –ë–∞–∑–æ–≤–∏–π –∑–∞–ø–∏—Ç
        where_conditions = [
            f"e.event_type = {PUSH_EVENT_TYPE}",
            f"e.type = {ANDROID_TYPE}",
            "d.gadid IS NOT NULL",
            "d.tag IS NOT NULL",
            f"toDate(e.created_at) >= '{start_date}'",
            f"toDate(e.created_at) <= '{end_date}'"
        ]
        
        # –î–æ–¥–∞—Ç–∫–æ–≤—ñ —Ñ—ñ–ª—å—Ç—Ä–∏
        if ab_groups:
            ab_filter = "', '".join(ab_groups)
            where_conditions.append(f"d.tag IN ('{ab_filter}')")
        
        if countries:
            country_filter = "', '".join(countries)
            where_conditions.append(f"d.country_name IN ('{country_filter}')")
        
        query = f"""
        SELECT 
            toString(d.gadid) as gadid,
            d.tag as ab_group,
            d.country_name as country,
            COUNT(*) as push_count,
            MIN(e.created_at) as first_push,
            MAX(e.created_at) as last_push,
            COUNT(DISTINCT toDate(e.created_at)) as push_days,
            AVG(e.sub_1) as avg_success_rate
        FROM event e
        JOIN device d ON e.device_id = d.id
        WHERE {' AND '.join(where_conditions)}
        GROUP BY gadid, ab_group, country
        ORDER BY push_count DESC
        """
        
        params = {
            'start_date': start_date,
            'end_date': end_date,
            'ab_groups': ab_groups,
            'countries': countries
        }
        
        return self.execute_query('statistic', query, params)
    
    def get_conversion_data(self,
                           start_date: str = CONVERSION_START_DATE,
                           end_date: str = CONVERSION_END_DATE,
                           campaign_ids: List[int] = None,
                           conversion_types: List[str] = ['deposit', 'registration']) -> pd.DataFrame:
        """
        –û—Ç—Ä–∏–º—É—î –¥–∞–Ω—ñ –ø—Ä–æ –∫–æ–Ω–≤–µ—Ä—Å—ñ—ó –∑ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—î—é
        
        Args:
            start_date: –ü–æ—á–∞—Ç–∫–æ–≤–∞ –¥–∞—Ç–∞
            end_date: –ö—ñ–Ω—Ü–µ–≤–∞ –¥–∞—Ç–∞
            campaign_ids: –°–ø–∏—Å–æ–∫ ID –∫–∞–º–ø–∞–Ω—ñ–π
            conversion_types: –¢–∏–ø–∏ –∫–æ–Ω–≤–µ—Ä—Å—ñ–π ('deposit', 'registration')
        
        Returns:
            DataFrame –∑ –∫–æ–Ω–≤–µ—Ä—Å—ñ—è–º–∏
        """
        where_conditions = [
            "sub_id_14 IS NOT NULL",
            "sub_id_14 != ''",
            f"date_key >= '{start_date}'",
            f"date_key <= '{end_date}'"
        ]
        
        # –§—ñ–ª—å—Ç—Ä –ø–æ –∫–∞–º–ø–∞–Ω—ñ—è—Ö
        if campaign_ids:
            ids_filter = ','.join(map(str, campaign_ids))
            where_conditions.append(f"campaign_id IN ({ids_filter})")
        
        # –§—ñ–ª—å—Ç—Ä –ø–æ —Ç–∏–ø–∞—Ö –∫–æ–Ω–≤–µ—Ä—Å—ñ–π
        conversion_filter = []
        if 'deposit' in conversion_types:
            conversion_filter.append("is_sale > 0")
        if 'registration' in conversion_types:
            conversion_filter.append("is_lead > 0")
        
        if conversion_filter:
            where_conditions.append(f"({' OR '.join(conversion_filter)})")
        
        query = f"""
        SELECT 
            sub_id_14 as gadid,
            SUM(is_sale) as total_deposits,
            SUM(is_lead) as total_registrations,
            MIN(datetime) as first_conversion,
            MAX(datetime) as last_conversion,
            COUNT(*) as conversion_events,
            SUM(sale_revenue) as total_revenue,
            country,
            campaign_id
        FROM keitaro_clicks
        WHERE {' AND '.join(where_conditions)}
        GROUP BY gadid, country, campaign_id
        """
        
        params = {
            'start_date': start_date,
            'end_date': end_date,
            'campaign_ids': campaign_ids,
            'conversion_types': conversion_types
        }
        
        return self.execute_query('keitaro', query, params)
    
    def get_campaign_info(self, app_names: List[str] = TARGET_APPS) -> pd.DataFrame:
        """
        –û—Ç—Ä–∏–º—É—î —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –∫–∞–º–ø–∞–Ω—ñ—ó —Ü—ñ–ª—å–æ–≤–∏—Ö –∑–∞—Å—Ç–æ—Å—É–Ω–∫—ñ–≤
        
        Args:
            app_names: –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤ –∑–∞—Å—Ç–æ—Å—É–Ω–∫—ñ–≤
        
        Returns:
            DataFrame –∑ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—î—é –ø—Ä–æ –∫–∞–º–ø–∞–Ω—ñ—ó
        """
        # –°—Ç–≤–æ—Ä—é—î–º–æ —É–º–æ–≤–∏ –ø–æ—à—É–∫—É –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∑–∞—Å—Ç–æ—Å—É–Ω–∫—É
        search_conditions = []
        for app in app_names:
            search_conditions.extend([
                f"name ILIKE '%{app}%'",
                f"alias ILIKE '%{app}%'"
            ])
        
        query = f"""
        SELECT DISTINCT 
            id,
            name,
            alias,
            state,
            created_at,
            updated_at
        FROM keitaro_campaigns
        WHERE ({' OR '.join(search_conditions)})
        ORDER BY name
        """
        
        params = {'app_names': app_names}
        
        return self.execute_query('keitaro', query, params)
    
    def get_device_info(self, gadids: List[str]) -> pd.DataFrame:
        """
        –û—Ç—Ä–∏–º—É—î –¥–æ–¥–∞—Ç–∫–æ–≤—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –ø—Ä–∏—Å—Ç—Ä–æ—ó
        
        Args:
            gadids: –°–ø–∏—Å–æ–∫ GADID –¥–ª—è –ø–æ—à—É–∫—É
        
        Returns:
            DataFrame –∑ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—î—é –ø—Ä–æ –ø—Ä–∏—Å—Ç—Ä–æ—ó
        """
        if not gadids:
            return pd.DataFrame()
        
        # –û–±–º–µ–∂—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è –∑–∞–Ω–∞–¥—Ç–æ –≤–µ–ª–∏–∫–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤
        gadids_sample = gadids[:10000] if len(gadids) > 10000 else gadids
        gadids_filter = "', '".join(gadids_sample)
        
        query = f"""
        SELECT 
            toString(gadid) as gadid,
            country_name,
            language_name,
            tag,
            type,
            active,
            timezone,
            created_at as device_created_at
        FROM device
        WHERE toString(gadid) IN ('{gadids_filter}')
        """
        
        params = {'gadids': len(gadids_sample)}
        
        return self.execute_query('statistic', query, params)
    
    def get_push_performance_summary(self) -> Dict[str, Any]:
        """
        –û—Ç—Ä–∏–º—É—î –∑–∞–≥–∞–ª—å–Ω—É —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ push-–∫–∞–º–ø–∞–Ω—ñ—è–º
        
        Returns:
            –°–ª–æ–≤–Ω–∏–∫ –∑ –∫–ª—é—á–æ–≤–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        logger.info("üìä –ó–±—ñ—Ä –∑–∞–≥–∞–ª—å–Ω–æ—ó —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏...")
        
        # Push —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        push_query = f"""
        SELECT 
            COUNT(DISTINCT d.gadid) as unique_users,
            COUNT(*) as total_pushes,
            COUNT(DISTINCT d.tag) as ab_groups_count,
            COUNT(DISTINCT d.country_name) as countries_count,
            AVG(e.sub_1) as avg_delivery_rate
        FROM event e
        JOIN device d ON e.device_id = d.id
        WHERE e.event_type = {PUSH_EVENT_TYPE}
          AND e.type = {ANDROID_TYPE}
          AND toDate(e.created_at) >= '{PUSH_START_DATE}'
          AND toDate(e.created_at) <= '{PUSH_END_DATE}'
        """
        
        push_stats = self.execute_query('statistic', push_query)
        
        # –ö–æ–Ω–≤–µ—Ä—Å—ñ—ó —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        conversion_query = f"""
        SELECT 
            COUNT(DISTINCT sub_id_14) as unique_converters,
            SUM(is_sale) as total_deposits,
            SUM(is_lead) as total_registrations,
            SUM(sale_revenue) as total_revenue
        FROM keitaro_clicks
        WHERE sub_id_14 IS NOT NULL
          AND date_key >= '{CONVERSION_START_DATE}'
          AND date_key <= '{CONVERSION_END_DATE}'
          AND (is_sale > 0 OR is_lead > 0)
        """
        
        conversion_stats = self.execute_query('keitaro', conversion_query)
        
        # –û–±'—î–¥–Ω—É—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        summary = {
            'period': f"{PUSH_START_DATE} - {PUSH_END_DATE}",
            'push_stats': push_stats.iloc[0].to_dict() if not push_stats.empty else {},
            'conversion_stats': conversion_stats.iloc[0].to_dict() if not conversion_stats.empty else {},
            'generated_at': datetime.now().isoformat()
        }
        
        return summary
    
    def clear_cache(self):
        """–û—á–∏—â–∞—î –∫–µ—à"""
        cache_dir = 'data/cache'
        if os.path.exists(cache_dir):
            for file in os.listdir(cache_dir):
                if file.endswith('.parquet'):
                    os.remove(os.path.join(cache_dir, file))
            logger.info("üóëÔ∏è –ö–µ—à –æ—á–∏—â–µ–Ω–æ")
    
    def get_cache_info(self) -> Dict[str, Any]:
        """–Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –∫–µ—à"""
        cache_dir = 'data/cache'
        if not os.path.exists(cache_dir):
            return {'files': 0, 'total_size': 0}
        
        files = [f for f in os.listdir(cache_dir) if f.endswith('.parquet')]
        total_size = sum(os.path.getsize(os.path.join(cache_dir, f)) for f in files)
        
        return {
            'files': len(files),
            'total_size_mb': round(total_size / 1024 / 1024, 2),
            'cache_enabled': self.cache_enabled
        }